{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our pipeline process:\n",
    "#### Extract Features:\n",
    "- `Airport_Pair` which will be used for `Delay_Trend_Past_Week` to see if there was a Delay trend in the previous week\n",
    "- `Same_Day_Tail_Reuse`which will be using **ONLY** the date from `dep_datetime`, as well as using `Tail_Number`\n",
    "- `Previous_Flight_Delay` which will be using information from `Tail_Number`, `dep_date` which will derive from `dep_datetime`, and `CRSDepTime`\n",
    "- `Turnaround_Time` will also be extracted from `CRSDepTime` subtracted by `Previous_Arrival_Time`(Which is derived from `Tail_Number`, `dep_date`, and `CRSArrTime`)\n",
    "- `Slack_Time` that derives from `CRSArrTime` subtracted by `CRSElaspedTime`\n",
    "\n",
    "#### Transformations: \n",
    "- `log_distance` using the log transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of your imports here (you may need to add some)\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output = \"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratification Function\n",
    "def stratified_random_split(df: pd.DataFrame, target_column: str, test_size: float = 0.1, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Performs a stratified random train-test split to ensure all classes in \n",
    "    'Flight_Status' are proportionally represented in both sets.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset containing the target variable.\n",
    "    target_column (str): The column representing the classification target.\n",
    "    test_size (float): The proportion of data to be used as test data.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (train_df, test_df) DataFrames.\n",
    "    \"\"\"\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, stratify=df[target_column], random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_df)} samples\")\n",
    "    print(f\"Test size: {len(test_df)} samples\")\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the DataFrame for the flight data\n",
    "flight_data = pd.read_parquet(\"data/WEATHER.parquet\")\n",
    "\n",
    "# Stratify the data\n",
    "train_data, test_data= stratified_random_split(flight_data, target_column=\"Flight_Status\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
